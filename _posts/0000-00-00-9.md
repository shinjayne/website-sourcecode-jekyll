---
#default
layout : post
title : "Auto Encoder 에 대하여 : Auto Encoder 의 개념과 사용법, 그리고 Regularization"
date : 2017-08-02 21:21:50 +0900
categories : deeplearning
tags : sklearn python

#custom
post_id : 9
permalink : "/post/9"
imageurl : "/asset/media/image/post/9"
mainpic : "/asset/media/image/post/9/1.png"
---

![](/images/ae/1.png)

Autoencoder 의 간략한 구조는 위와 같다.

=> self supervised learning 에 가깝다.

자기 자신을 target(label) 값으로 사용하기 때문이다.

가운데 초록색 노드들(Hidden nodes) 을 coding layer, 또는 bottleneck 이라고 부른다.  

<br/>
<br/>
# 1. Autoencoder 를 어디에 쓰나
<hr/>
<br/>
**1.feature detection **

학습된 autoencoder에서는 coding layer 가 input 의 중요한 feature 들을 나타내게된다.

**2.recommendation system**

추후 튜토리얼을 확인하자.

**3.encoding **

Decoder로 디코딩할수 있는 코드로 input 값을 encoding(암호화) 하는 기능을 할 수 있다.

<br/>
<br/>
# 2. Autoencoder 의 작동
<hr/>
<br/>

![](/images/ae/3.png)


[이 사이트](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/) 를 참고한 모델을 설명한다.

주로 activation function 으로 tanh 를 사용한다. (hidden 직전과 visible 직전에)

output 값에 softmax 를 사용한다.

<br/>
<br/>
# 3. Autoencoder 의 Bias
<hr/>
<br/>

![](/images/ae/5.png)

이렇게 Bias 를 추가할 수도 있고, 그림으론 이렇게 표현한다.

<br/>
<br/>
# 4. Autoencoder 의 Training
<hr/>
<br/>

|step|내용|부가|
|:--:|:--|:--|
|1|input vector x_u = (r1,r2,.., ri,...,rm) 은 user u 의 m개의 영화에 대한 rating이다.|rating은 1~5 사이의 값이고, rating 없으면 0이다.|
|2|input vector(user 단위) 가 하나 network 로 들어간다.||
|3|input vector x_u 는 vector z 로 인코딩된다.x 에서 z 로 갈 때 mapping function 에 의해 차원이 축소된다.| mapping function : z = f(Wx+b) (f : sigmoid, tanh , etc)|
|4|z 가 output vector y로 decoding 된다. y는 x vector 와 같은 차원을 가진다. |y가 x의 복사본이 되게 하는 것이 학습의 목적이다.|
|5|reconstruction error d(x,y) 를 계산한다. 이 error function 을 최소화시킨다.|error function : d(x,y)=//x-y// |
|6|back-propagation 을 이용해, error 의 값이 역전파되고, W,b 값들이 tuning 된다.|learning rate 에 따라 학습 정도가 달라진다.|
|7|step 1~6 을 반복하면서 파라메터들을 업데이트한다. |만약 vector 하나씩 넣으면서 update 시키면 Reinforcement Learning 이고, 여러 batch 씩 한꺼번에 넣으면서 학습시키면 Batch Learning |
|8|전체 데이터셋을 한번 다 학습시켰다면, epoch 단위로 몇번 더 학습한다.||




<br/>
<br/>
# 5. Hidden Layer 의 갯수가 많을수록 좋을까?
<hr/>
<br/>
![](/images/ae/8.jpeg)

위와같이 hidden node 의 갯수를 input 갯수보다 늘리면 좋을까?

언뜻 생각하면, 더 많은 feature를 뽑아낼 수 있어서 좋겠다 라고 생각할 수 있지만 실제론 그렇지 않다.

맨 상단과 맨 아래 있는 node 가 사용되지 않고, 가운데 5개의 node 가 input 을 output  으로 그대로 복사하는 문제가 생긴다.

encoding 의 기능이 없어지는 것이다. 그렇게되면 의미있는 feature 를 뽑아낼 수 없다.

이러한 문제, 즉 Autoencoder 의 Overfitting(과적합) 문제를 해결하기 위한 Regularization 방법들을 소개한다.

Sparse AE,



<br/>
<br/>
# 6. Regularization1 : Sparse Autoencoder
<hr/>
<br/>

매우 유명하다. 굉장히 많이 사용된다.

위처럼 데이터를 그대로 옮기는 hidden node 가 생기는  overfitting  문제를 해결한다.

간단히 말하자면 hidden node 중 매번 일부 node 만 사용해서 학습한다.

![](/images/ae/9.jpeg)



<br/>
<br/>
# 7. Regularization2 : Denoising Autoencoder
<hr/>
<br/>

Stochastic 한 Auto Encoder 다.

input X 중의 일부만 사용한다.

![](/images/ae/10.jpeg)



<br/>
<br/>
# 8. Regularization3 : Contractive Autoencoder
<hr/>
<br/>

정리 후 내용을 추후 추가하겠다.

<br/>
<br/>
# 9. Stacked Autoencoder
<hr/>
<br/>

Stacked Autoencoder 는 간단히 encoding layer를 하나 더 추가한 것인데, 성능은 매우 강력하다.

이 간단한 모델이 Deep Belief Network 의 성능을 넘어서는 경우도 있다고 하니, 정말 대단하다.

참고자료를 읽고, 다시 정리하겠다.

![](/images/ae/11.jpeg)

<br/>
<br/>
# 10. Deep Autoencoder
<hr/>
<br/>

Deep autoencoder 를 알기 전에 확실하게 짚고 넘어가야할 부분은, **Deep Autoencoder 와 Stacked Autoencoder 는 전혀 다른것이다. **

반드시 구분해야한다.

Deep autoencoder 는 RBM ( Ristricted Boltzman Machine ) 을 쌓아 만들었고,

Stacked autoencoder 는 autoencoder 의 encoding layer 를 깊게 쌓아 만든 것이다.



# Python 의 상속

# 강제상속 ? 자동상속?

강제상속 : 나에게 없을때, 부모클래스 namespace 로 거슬러 올라가 강제 사용

자동상속 : 내가 부모 클래스 메서드를 덮어썼지만, 부모클래스메서드를 사용하고 싶을 때 super(\__class\__, self).method() 로 부모메서드 호출


```python
class Base :
    def __init__(self, name) :
        self.name = name

class Sub1(Base):
    pass

class Sub2(Base):
    def __init__(self, name, tall, age) :
        super(Sub2, self).__init__(name)
        self.tall = tall
        self.age = age

```


```python
#자동상속
#s1 은 __init__ 이 없어서 Base 것을 참조한다
s1 = Sub1("jayne")
#강제상속
#s2 는 자체 __init__ 이 있어서 Base 것을 덮어쓴다. Base 것을 강제 이용하려면, super를 통해 강제상속시킬 수 있다.
s2 = Sub2("jayne",182, 22)
```

# 다중상속시 super 는 누구것을 가져오나?


```python
class Base :
    def __init__(self, name) :
        self.name = name

class Sub1(Base):
    def __init__(self, name) :
        self.name = name

class Sub2(Base):
    def __init__(self, name, tall, age) :
        super(Sub2, self).__init__(name)
        self.tall = tall
        self.age = age



class SubSub(Sub1, Sub2):
    def __init__(self, name) :
        super(SubSub, self).__init__(name)
```

위처럼 코드를 만들면, super는 누구의 init 메서드를 강제상속할까.

후보는 Sub1 과 Sub2 이다.

후보1 : Sub2 의 메서드를 강제 상속하므로 인자로 name, tall, age 모두 넘겨줘야한다.

후보2 : Sub1 의 메서드를 찾다가, Base 의 init 메서드를 강제상속해오므로, name 만 넘겨주면 된다.

이 답을 알아보는 방법은 help() 함수이다.


```python
ss = SubSub("so")
```


```python
help(ss)
```

    Help on SubSub in module __main__ object:

    class SubSub(Sub1, Sub2)
     |  Method resolution order:
     |      SubSub
     |      Sub1
     |      Sub2
     |      Base
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  __init__(self, name)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from Base:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)



Method resolution order 를 보면 알 있다.

SubSub 는 자신에게 메서드가 없을 때(자동상속)나 부모의 메서드를 super 로 가져올 때 (강제상속) 위의 ordering 을 따르므로

`super().__init__()` 을 호출 할 때에도, Sub1 에 `__init__` 이 있기에  그 init 부터 가져와 사용하게 된다.

# 결론 : 모든것은 namespace 참조 순서에 달렸다.


```python

```
